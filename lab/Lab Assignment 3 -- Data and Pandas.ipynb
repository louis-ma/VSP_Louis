{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f34e69",
   "metadata": {},
   "source": [
    "# Lab Assignment 3 -- Data and Pandas\n",
    "In this lab, you will complete a series of exercises related to the lecture material on data and Pandas using some real datasets. Each exercise will focus around a single dataset and contain multiple steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b004e6da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# We import all of the libraries you will need\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# We import all of the libraries you will need\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a79149",
   "metadata": {},
   "source": [
    "## Exercise 1 -- Unemployment Data\n",
    "Below, we load in data on Unemployment in the United States at the State level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Not Edit\n",
    "url = \"https://datascience.quantecon.org/assets/data/state_unemployment.csv\"\n",
    "unemp = pd.read_csv(url, parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c01d47d",
   "metadata": {},
   "source": [
    "## Exercise 1a -- Displaying Data\n",
    "Complete the following steps:\n",
    "- In two cells below, display the top 7 rows  and the bottom 3 rows of `unemp`. \n",
    "- In the third cell, change `unemp` so its column names are strictly lowercase\n",
    "- Display the resulting DataFrame by calling `unemp` at the bottomg of the third cell.\n",
    "\n",
    "From a \"tidy\" perspective, what is an observation in this data? Explain why. Answer in the Markdown cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b3c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 1a -- Top 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6da9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Exercise 1a -- Bottom 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec83638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1a -- Rename Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53957b16",
   "metadata": {},
   "source": [
    "### Response to Exercise 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517e89e",
   "metadata": {},
   "source": [
    "## Exercise 1b -- Creating Variables & Index Setting\n",
    "Complete the following steps:\n",
    "- Create a column in `unemp` called \"year\" that is equal to the year of the date. \n",
    "- Change the index of `unemp` so that the index (or indices) reflect the observational units. \n",
    "- Display the indices.  \n",
    "\n",
    "In the Markdown cell below, address the following prompts:\n",
    "1. How many indices are there?\n",
    "2. Why are there this many indices? Write an equation that explains it.\n",
    "3. Is one of your indices a `DateTimeIndex` object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf7d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1b Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0aa8fe",
   "metadata": {},
   "source": [
    "### Response to Exercise 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037848f",
   "metadata": {},
   "source": [
    "### Exercise 1c -- Plotting Annual Averages\n",
    "Complete the following steps:\n",
    "1. Using `tiny_unemp`, find the year-state average of the unemployment rate and save it to `yearly_state_unemp`.\n",
    "2. Reshape `yearly_state_unemp` by using `unstack()`. Ensure that your row indices are years and your column variables are states.  \n",
    "3. Display `yearly_state_unemp` by making it the last line in the cell.\n",
    "4. Note that the `unemploymentrate` level is not very useful because all of the numbers are unemployment rates. Let's remove it by using `.droplevel()` on `yearly_state_unemp`. Call this new DataFrame `clean_state_unemp` You may want to reference the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.droplevel.html) to see which argument you will need. \n",
    "5. Use the `.plot()` method on `clean_state_unemp`.\n",
    "6. In the next cell, use the `.plot()` method on `yearly_state_unemp`.\n",
    "\n",
    "In the Markdown cell below **answer the following questions**: \n",
    "1. What is the most salient real world phenomenon this is visible in the plot?\n",
    "2. Compare the two plots. How did removing the `unemploymentrate` level change the plot?\n",
    "\n",
    "**Hints**\n",
    "- You can use `df.drop()` ([documentation here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html))  to get rid of the `laborforce` column when creating `yearly_state_unemp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8819ab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Colorado', 'California', 'Alabama', 'New York', 'Florida'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Do not edit this code\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tiny_unemp \u001b[38;5;241m=\u001b[39m \u001b[43munemp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColorado\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCalifornia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlabama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNew York\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFlorida\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\quntecon2\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\quntecon2\\lib\\site-packages\\pandas\\core\\indexing.py:1301\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\quntecon2\\lib\\site-packages\\pandas\\core\\indexing.py:1239\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1241\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\quntecon2\\lib\\site-packages\\pandas\\core\\indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1430\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1432\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\quntecon2\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6111\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6109\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6111\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6113\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6115\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\quntecon2\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6171\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6170\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6173\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6174\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Colorado', 'California', 'Alabama', 'New York', 'Florida'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "# Do not edit this code\n",
    "tiny_unemp = unemp.loc[[\"Colorado\", \"California\", \"Alabama\", \"New York\", \"Florida\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964bec9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1c -- Steps 1-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42aa27f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1c -- Steps 4 & 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1c -- Step 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5c33c",
   "metadata": {},
   "source": [
    "### Response to Exercise 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96daf06e",
   "metadata": {},
   "source": [
    "## Exercise 1d -- Using Lags to Plot Difference\n",
    "Instead of plotting the unemployment rate average over time, we will plot the annual difference between unemployment rates over time for each state. To do this, complete the following steps:\n",
    "1. Create a DataFrame called `shifted_unemp` and assign it to `clean_state_unemp` shifted by 1.\n",
    "2. Create a DataFrame called `change_unemp` and assign it to the the difference between `clean_state_unemp` and `shifted_unemp` divided by `shifted_unemp`.\n",
    "3. Call `.plot()` on `change_unemp`.\n",
    "\n",
    "Answer the following questions in the markdown cell below. \n",
    "1. Why is the year 2000 no longer being plotted? Look at the `change_unemp` DataFrame if you are unsure.\n",
    "2. Which state saw the largest annual increase in unemployment during this period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee44c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1d code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6be3a0",
   "metadata": {},
   "source": [
    "### Response to Exercise 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3f612",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "In this question, we're going to look at data on daily Covid cases in British Columbia from the [COVID-19 Canada Open Data Working Group](https://github.com/ccodwg/Covid19Canada). This data is broken down into five health regions:\n",
    "- Fraser Health (Fraser)\n",
    "- Interior Health (Interior)\n",
    "- Northern Health (Northern)\n",
    "- Vancouver Coastal Health (Vancouver Coastal)\n",
    "- Vancouver Island Health Authority (Island)\n",
    "\n",
    "You can see the geography of these regions below (Image from gov.bc.ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d47643b",
   "metadata": {},
   "source": [
    "<img src = \"https://www2.gov.bc.ca/assets/gov/health/managing-your-health/mental-health-substance-use/find-services-map-large.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531aed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Dowlonad the Data -- don't edit this cell\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cases_bc \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdata\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mcovid_cases_bc.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m cases_bc\u001b[39m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Dowlonad the Data -- don't edit this cell\n",
    "cases_bc = pd.read_csv(\"..\\\\data\\\\covid_cases_bc.csv\")\n",
    "cases_bc.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd4dea",
   "metadata": {},
   "source": [
    "## Exercise 2a -- Checking the Data\n",
    "`cases_bc` contains daily reported covid cases per hundred thousand people in BC for the year 2021. The data is broken down by health region. Before working with the data, you should learn how its structured and check it for potential errors. Answer the following questions in the first markdown cell below.\n",
    "1. What is an observation (or row) in the dataset?\n",
    "2. What is the index? Could we turn one of our variables into an index? Which one?\n",
    "3. Does it make sense for any of the values in the table to be negative? Why or why not?\n",
    "\n",
    "Now complete the following steps:\n",
    "- Make it so the date is our index and check whether it is a `DateTimeIndex`. Afterwards, display `cases_bc` by making it the last line of the cell below. \n",
    "- In the second cell, call `.dropna()` on cases_bc. \n",
    "- In the third cell, us `.any()` and comparison operators **applied to `cases_bc`** to display all dates on which at least one of the health regions has a negative value. Call this DataFrame `neg_values`. Display `neg_values` by making it the last line of the cell. (**Hint**: Observe what happens when you use a comparison operator on a DataFrame. You can use the output to easily form an index for your DataFrame.)\n",
    "- If any dates had a negative values, use comparison operators to set all negative values in `cases_bc` to `NaN`.\n",
    "\n",
    "\n",
    "Answer the following Questions in the second Markdown cell.\n",
    "\n",
    "4. Did dropping missing value change the DataFrame at all?\n",
    "5. Did you find any negative values? If so, they have been turned into `NaN` values and will not be used in future analysis. What would be a possible alternative to this approach?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4d351",
   "metadata": {},
   "source": [
    "### Response to Exercise 2a Questions 1-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e1add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2a -- Step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8472f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2a -- Step 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af050da4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exercise 2a -- Step 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a8a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2a -- Step 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d5242",
   "metadata": {},
   "source": [
    "### Response to Exercise 2a 4-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d78f12",
   "metadata": {},
   "source": [
    "## Exercise 2b -- Aggregations\n",
    "Using aggregators with the axis argument, complete the following steps:\n",
    "1. At each date, find the minimum number of cases per 100,000 across health regions. Print the top 3 rows of the resulting series.\n",
    "2. For each health region, what was the median number of daily cases per 100,000 in 2021. Print the resulting series. (**Hint:** Think about how long should this series be.)\n",
    "3. What the maximum number of daily cases per 100,000 across all health regions? Which health region did this maximum occur in? What day was the maximum attained?\n",
    "\n",
    "**Hint:** For the last step, you might need to aggregate twice. You may also want to use the aggregator `.idxmax()` which returns the index of the maximum value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa16b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2b -- Step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2b -- Step 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6cb53c",
   "metadata": {},
   "source": [
    "## Exercise 2c -- Classifying Variance\n",
    "Averages and medians communicate some notion of the statistical center of data. Similarly, the sample variance of data gives us some sense of how dispersed the data is around that center. A low variance means the data is relatively concentrated whereas a high variance means the data is relatively dispersed. The sample variance of a column of data $x$ can be given by the following equation\n",
    "$$\n",
    "var(x) = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2\n",
    "$$\n",
    "where $n$ is the nimber of elements in $x$, $\\bar{x}$ is the average of $x$, and $x_i$ is a single element in $x$. Complete the following steps:\n",
    "1.  The method DataFrame and Series method `.var()` will automatically calculate the variance of each column or row in a DataFrame. Use `.var()` to calculate the variance in the daily cases per 100,000 for each health region in 2021. Name this DataFrame `hr_var`.\n",
    "2. Currently,  `hr_var` has a single column indexed by the integer 0. Rename this column so it is called \"s_var\".\n",
    "3. Using a list comprehension, create a new column in `hr_var` called \"classification\" which is equal to\n",
    "    - \"High\" if the variance for a health region is strictly greater than 300,  \n",
    "    - \"Medium\" if the variance for a health region is strictly greater than 150 and less than or equal to 300\n",
    "    - \"Low\" if the variance for a health region is less than or equal to 150. \n",
    "    \n",
    "4. Display the resulting DataFrame by having `hr_var` as your last line in the cell. \n",
    "5. In the second cell, use `cases_bc`, indexing, and `.plot()` to plot the daily cases per 100,000 for the Northern and Island health regions.\n",
    "\n",
    "Finally, in the Markdown cell below **answer the following question**. What are the classifications for the Northern and Island regions. What features of the two lines you plotted reflect these classifications? \n",
    "\n",
    "\n",
    "**Hint:** For step 3, you will want to use a nested if else statements within the list comprehension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244840b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2c -- Steps 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71740a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2c -- Step 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d92cc",
   "metadata": {},
   "source": [
    "### Response to Exercise 2c "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6e22e",
   "metadata": {},
   "source": [
    "## Exerice 2d -- More Classifying & `.applymap()`\n",
    "Now, we want to determine whether the cases per 100,000 on a given day was \"High\" ($> 10$), \"Low\"($\\leq 10$ and $>0$), or \"None\" ($=0$) for each region-day. To do this, complete the following steps:\n",
    "1. Define a function called `classify_cases` that takes a **single number** and returns a \"High\", \"Low\" or \"None\" according to the criteria aboce\n",
    "2. `.applymap()` takes a function of a single value and applies that function to each cell in a DataFrame  individuall. Using `classify_cases` and `.applymap`, create a DataFrame that has a classification for each region-day. Call this DataFrame `cases_bins`. \n",
    "3. Print the top 5 rows of that DataFratme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca95753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2d code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2a264",
   "metadata": {},
   "source": [
    "### Exercise 2e -- Classification Count\n",
    "Next, we want to count how many days of each type (\"High\", \"Low\", and \"None\") each helth region had on 2021. Complete the following steps:\n",
    "1. Using `pd.value_counts` and `.apply()`, create DataFrame called `class_counts` where the row indices are the three classes and the columns are health regions.\n",
    "2. Using the DataFrame method `.barh()`, create a horizontal bar plot where there are five groups of three bars. \n",
    "\n",
    "**Hint:** You may have to use `.T` to get the right bar chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677a256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2e code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4be232",
   "metadata": {},
   "source": [
    "### Exercise 2f -- Choose Your Health Region\n",
    "Choose one of the five health regions. Complete the following steps to find the average number of cases per 100,000 when case loads are \"High\" and when case loads are \"Low\".\n",
    "1. Using `pd.concat()` `cases_bc`, and `cases_bin`, create a DataFrame with two columns called `my_health_region`. The first column should be titled \"cases\" and include the number of cases per 100,000 in your chosen health region for each day. The second column should be titled \"class\" and include the classification for that health region in each day.\n",
    "2. Using `.groupby()` and an aggregator, find the average number of cases per 100,000 when case loads are \"High\" and when case loads are \"Low\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b17004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2f code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
